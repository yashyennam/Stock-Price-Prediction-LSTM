{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashyennam/Stock-Price-Prediction-LSTM/blob/master/trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJr_9dXGpJ05",
        "outputId": "71106eb6-d80a-4193-8998-acbdec8ec9ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# IMPORTING IMPORTANT LIBRARIES\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIQ58pGb9kfI",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-length",
                  "5492"
                ],
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "1a124c19-ef74-4d88-e192-173700c09d4b"
      },
      "source": [
        "# FOR REPRODUCIBILITY\n",
        "np.random.seed(7)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "# IMPORTING DATASET \n",
        "# dataset = pd.read_csv('apple_share_price.csv', usecols=[1,2,3,4])\n",
        "\n",
        "import io\n",
        "dataset = pd.read_csv(io.BytesIO(uploaded['apple_share_price.csv']), usecols= [1, 2, 3, 4])\n",
        "dataset = dataset.reindex(index = dataset.index[::-1])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c4c48226-348e-4079-9e4e-534bd04f42f9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c4c48226-348e-4079-9e4e-534bd04f42f9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving apple_share_price.csv to apple_share_price.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYM9GsNm-NHf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "34383934-ef3d-493c-acc3-3e17e7d24698"
      },
      "source": [
        "# CREATING OWN INDEX FOR FLEXIBILITY\n",
        "obs = np.arange(1, len(dataset) + 1, 1)\n",
        "\n",
        "# TAKING DIFFERENT INDICATORS FOR PREDICTION\n",
        "OHLC_avg = dataset.mean(axis = 1)\n",
        "HLC_avg = dataset[['High', 'Low', 'Close']].mean(axis = 1)\n",
        "close_val = dataset[['Close']]\n",
        "\n",
        "# PLOTTING ALL INDICATORS IN ONE PLOT\n",
        "plt.plot(obs, OHLC_avg, 'r', label = 'OHLC avg')\n",
        "plt.plot(obs, HLC_avg, 'b', label = 'HLC avg')\n",
        "plt.plot(obs, close_val, 'g', label = 'Closing price')\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.show()\n",
        "\n",
        "# PREPARATION OF TIME SERIES DATASE\n",
        "OHLC_avg = np.reshape(OHLC_avg.values, (len(OHLC_avg),1)) # 1664\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "OHLC_avg = scaler.fit_transform(OHLC_avg)\n",
        "\n",
        "# TRAIN-TEST SPLIT\n",
        "train_OHLC = int(len(OHLC_avg) * 0.75)\n",
        "test_OHLC = len(OHLC_avg) - train_OHLC\n",
        "train_OHLC, test_OHLC = OHLC_avg[0:train_OHLC,:], OHLC_avg[train_OHLC:len(OHLC_avg),:]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSO5BEARtSCT3IAiCNuRfmdC2Vn/B4m0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP5Kr0y_BXPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f26f4334-97b7-4df3-e217-ff32a1f16da2"
      },
      "source": [
        "help(model)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on Sequential in module keras.engine.sequential object:\n",
            "\n",
            "class Sequential(keras.engine.training.Model)\n",
            " |  Linear stack of layers.\n",
            " |  \n",
            " |  # Arguments\n",
            " |      layers: list of layers to add to the model.\n",
            " |      name: Name given to the model\n",
            " |  \n",
            " |  # Example\n",
            " |  \n",
            " |  ```python\n",
            " |  # Optionally, the first layer can receive an `input_shape` argument:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32, input_shape=(500,)))\n",
            " |  \n",
            " |  # Afterwards, we do automatic shape inference:\n",
            " |  model.add(Dense(32))\n",
            " |  \n",
            " |  # This is identical to the following:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32, input_dim=500))\n",
            " |  \n",
            " |  # And to the following:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32, batch_input_shape=(None, 500)))\n",
            " |  \n",
            " |  # Note that you can also omit the `input_shape` argument:\n",
            " |  # In that case the model gets built the first time you call `fit` (or other\n",
            " |  # training and evaluation methods).\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32))\n",
            " |  model.add(Dense(32))\n",
            " |  model.compile(optimizer=optimizer, loss=loss)\n",
            " |  \n",
            " |  # This builds the model for the first time:\n",
            " |  model.fit(x, y, batch_size=32, epochs=10)\n",
            " |  \n",
            " |  # Note that when using this delayed-build pattern\n",
            " |  # (no input shape specified),\n",
            " |  # the model doesn't have any weights until the first call\n",
            " |  # to a training/evaluation method (since it isn't yet built):\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32))\n",
            " |  model.add(Dense(32))\n",
            " |  model.weights  # returns []\n",
            " |  \n",
            " |  # Whereas if you specify the input shape, the model gets built continuously\n",
            " |  # as you are adding layers:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32, input_shape=(500,)))\n",
            " |  model.add(Dense(32))\n",
            " |  model.weights  # returns list of length 4\n",
            " |  \n",
            " |  # When using the delayed-build pattern (no input shape specified), you can\n",
            " |  # choose to manually build your model by calling\n",
            " |  # `build(batch_input_shape)`:\n",
            " |  model = Sequential()\n",
            " |  model.add(Dense(32))\n",
            " |  model.add(Dense(32))\n",
            " |  model.build((None, 500))\n",
            " |  model.weights  # returns list of length 4\n",
            " |  ```\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Sequential\n",
            " |      keras.engine.training.Model\n",
            " |      keras.engine.network.Network\n",
            " |      keras.engine.base_layer.Layer\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, layers=None, name=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  add(self, layer)\n",
            " |      Adds a layer instance on top of the layer stack.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          layer: layer instance.\n",
            " |      \n",
            " |      # Raises\n",
            " |          TypeError: If `layer` is not a layer instance.\n",
            " |          ValueError: In case the `layer` argument does not\n",
            " |              know its input shape.\n",
            " |          ValueError: In case the `layer` argument has\n",
            " |              multiple output tensors, or is already connected\n",
            " |              somewhere else (forbidden in `Sequential` models).\n",
            " |  \n",
            " |  build(self, input_shape=None)\n",
            " |      Creates the layer weights.\n",
            " |      \n",
            " |      Must be implemented on all layers that have weights.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          input_shape: Keras tensor (future input to layer)\n",
            " |              or list/tuple of Keras tensors to reference\n",
            " |              for weight shape computations.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      # Returns\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  pop(self)\n",
            " |      Removes the last layer in the model.\n",
            " |      \n",
            " |      # Raises\n",
            " |          TypeError: if there are no layers in the model.\n",
            " |  \n",
            " |  predict_classes(self, x, batch_size=32, verbose=0)\n",
            " |      Generate class predictions for the input samples.\n",
            " |      \n",
            " |      The input samples are processed batch by batch.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: input data, as a Numpy array or list of Numpy arrays\n",
            " |              (if the model has multiple inputs).\n",
            " |          batch_size: integer.\n",
            " |          verbose: verbosity mode, 0 or 1.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A numpy array of class predictions.\n",
            " |  \n",
            " |  predict_proba(self, x, batch_size=32, verbose=0)\n",
            " |      Generates class probability predictions for the input samples.\n",
            " |      \n",
            " |      The input samples are processed batch by batch.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: input data, as a Numpy array or list of Numpy arrays\n",
            " |              (if the model has multiple inputs).\n",
            " |          batch_size: integer.\n",
            " |          verbose: verbosity mode, 0 or 1.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A Numpy array of probability predictions.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_config(config, custom_objects=None) from builtins.type\n",
            " |      Instantiates a Model from its config (output of `get_config()`).\n",
            " |      \n",
            " |      # Arguments\n",
            " |          config: Model config dictionary.\n",
            " |          custom_objects: Optional dictionary mapping names\n",
            " |              (strings) to custom classes or functions to be\n",
            " |              considered during deserialization.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A model instance.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case of improperly formatted config dict.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  layers\n",
            " |  \n",
            " |  model\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.engine.training.Model:\n",
            " |  \n",
            " |  compile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)\n",
            " |      Configures the model for training.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          optimizer: String (name of optimizer) or optimizer instance.\n",
            " |              See [optimizers](/optimizers).\n",
            " |          loss: String (name of objective function) or objective function.\n",
            " |              See [losses](/losses).\n",
            " |              If the model has multiple outputs, you can use a different loss\n",
            " |              on each output by passing a dictionary or a list of losses.\n",
            " |              The loss value that will be minimized by the model\n",
            " |              will then be the sum of all individual losses.\n",
            " |          metrics: List of metrics to be evaluated by the model\n",
            " |              during training and testing.\n",
            " |              Typically you will use `metrics=['accuracy']`.\n",
            " |              To specify different metrics for different outputs of a\n",
            " |              multi-output model, you could also pass a dictionary,\n",
            " |              such as `metrics={'output_a': 'accuracy'}`.\n",
            " |          loss_weights: Optional list or dictionary specifying scalar\n",
            " |              coefficients (Python floats) to weight the loss contributions\n",
            " |              of different model outputs.\n",
            " |              The loss value that will be minimized by the model\n",
            " |              will then be the *weighted sum* of all individual losses,\n",
            " |              weighted by the `loss_weights` coefficients.\n",
            " |              If a list, it is expected to have a 1:1 mapping\n",
            " |              to the model's outputs. If a dict, it is expected to map\n",
            " |              output names (strings) to scalar coefficients.\n",
            " |          sample_weight_mode: If you need to do timestep-wise\n",
            " |              sample weighting (2D weights), set this to `\"temporal\"`.\n",
            " |              `None` defaults to sample-wise weights (1D).\n",
            " |              If the model has multiple outputs, you can use a different\n",
            " |              `sample_weight_mode` on each output by passing a\n",
            " |              dictionary or a list of modes.\n",
            " |          weighted_metrics: List of metrics to be evaluated and weighted\n",
            " |              by sample_weight or class_weight during training and testing.\n",
            " |          target_tensors: By default, Keras will create placeholders for the\n",
            " |              model's target, which will be fed with the target data during\n",
            " |              training. If instead you would like to use your own\n",
            " |              target tensors (in turn, Keras will not expect external\n",
            " |              Numpy data for these targets at training time), you\n",
            " |              can specify them via the `target_tensors` argument. It can be\n",
            " |              a single tensor (for a single-output model), a list of tensors,\n",
            " |              or a dict mapping output names to target tensors.\n",
            " |          **kwargs: When using the Theano/CNTK backends, these arguments\n",
            " |              are passed into `K.function`.\n",
            " |              When using the TensorFlow backend,\n",
            " |              these arguments are passed into `tf.Session.run`.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case of invalid arguments for\n",
            " |              `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
            " |  \n",
            " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Returns the loss value & metrics values for the model in test mode.\n",
            " |      \n",
            " |      Computation is done in batches.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: Input data. It could be:\n",
            " |              - A Numpy array (or array-like), or a list of arrays\n",
            " |                (in case the model has multiple inputs).\n",
            " |              - A dict mapping input names to the corresponding\n",
            " |                array/tensors, if the model has named inputs.\n",
            " |              - A generator or `keras.utils.Sequence` returning\n",
            " |                `(inputs, targets)` or `(inputs, targets, sample weights)`.\n",
            " |              - None (default) if feeding from framework-native\n",
            " |                tensors (e.g. TensorFlow data tensors).\n",
            " |          y: Target data. Like the input data `x`,\n",
            " |              it could be either Numpy array(s), framework-native tensor(s),\n",
            " |              list of Numpy arrays (if the model has multiple outputs) or\n",
            " |              None (default) if feeding from framework-native tensors\n",
            " |              (e.g. TensorFlow data tensors).\n",
            " |              If output layers in the model are named, you can also pass a\n",
            " |              dictionary mapping output names to Numpy arrays.\n",
            " |              If `x` is a generator, or `keras.utils.Sequence` instance,\n",
            " |              `y` should not be specified (since targets will be obtained\n",
            " |              from `x`).\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per gradient update.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` is your data is in the\n",
            " |              form of symbolic tensors, generators, or\n",
            " |              `keras.utils.Sequence` instances (since they generate batches).\n",
            " |          verbose: 0 or 1. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar.\n",
            " |          sample_weight: Optional Numpy array of weights for\n",
            " |              the test samples, used for weighting the loss function.\n",
            " |              You can either pass a flat (1D)\n",
            " |              Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples),\n",
            " |              or in the case of temporal data,\n",
            " |              you can pass a 2D array with shape\n",
            " |              `(samples, sequence_length)`,\n",
            " |              to apply a different weight to every timestep of every sample.\n",
            " |              In this case you should make sure to specify\n",
            " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
            " |          steps: Integer or `None`.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              before declaring the evaluation round finished.\n",
            " |              Ignored with the default value of `None`.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during evaluation.\n",
            " |              See [callbacks](/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up when using\n",
            " |              process-based threading. If unspecified, `workers` will default\n",
            " |              to 1. If 0, will execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: in case of invalid arguments.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |  \n",
            " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Evaluates the model on a data generator.\n",
            " |      \n",
            " |      The generator should return the same kind of data\n",
            " |      as accepted by `test_on_batch`.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          generator: Generator yielding tuples (inputs, targets)\n",
            " |              or (inputs, targets, sample_weights)\n",
            " |              or an instance of Sequence (keras.utils.Sequence)\n",
            " |              object in order to avoid duplicate data\n",
            " |              when using multiprocessing.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              to yield from `generator` before stopping.\n",
            " |              Optional for `Sequence`: if unspecified, will use\n",
            " |              the `len(generator)` as a number of steps.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training.\n",
            " |              See [callbacks](/callbacks).\n",
            " |          max_queue_size: maximum size for the generator queue\n",
            " |          workers: Integer. Maximum number of processes to spin up\n",
            " |              when using process based threading.\n",
            " |              If unspecified, `workers` will default to 1. If 0, will\n",
            " |              execute the generator on the main thread.\n",
            " |          use_multiprocessing: if True, use process based threading.\n",
            " |              Note that because\n",
            " |              this implementation relies on multiprocessing,\n",
            " |              you should not pass\n",
            " |              non picklable arguments to the generator\n",
            " |              as they can't be passed\n",
            " |              easily to children processes.\n",
            " |          verbose: verbosity mode, 0 or 1.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case the generator yields\n",
            " |              data in an invalid format.\n",
            " |  \n",
            " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False, **kwargs)\n",
            " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: Input data. It could be:\n",
            " |              - A Numpy array (or array-like), or a list of arrays\n",
            " |                (in case the model has multiple inputs).\n",
            " |              - A dict mapping input names to the corresponding\n",
            " |                array/tensors, if the model has named inputs.\n",
            " |              - A generator or `keras.utils.Sequence` returning\n",
            " |                `(inputs, targets)` or `(inputs, targets, sample weights)`.\n",
            " |              - None (default) if feeding from framework-native\n",
            " |                tensors (e.g. TensorFlow data tensors).\n",
            " |          y: Target data. Like the input data `x`,\n",
            " |              it could be either Numpy array(s), framework-native tensor(s),\n",
            " |              list of Numpy arrays (if the model has multiple outputs) or\n",
            " |              None (default) if feeding from framework-native tensors\n",
            " |              (e.g. TensorFlow data tensors).\n",
            " |              If output layers in the model are named, you can also pass a\n",
            " |              dictionary mapping output names to Numpy arrays.\n",
            " |              If `x` is a generator, or `keras.utils.Sequence` instance,\n",
            " |              `y` should not be specified (since targets will be obtained\n",
            " |              from `x`).\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per gradient update.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of symbolic tensors, generators, or `Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          epochs: Integer. Number of epochs to train the model.\n",
            " |              An epoch is an iteration over the entire `x` and `y`\n",
            " |              data provided.\n",
            " |              Note that in conjunction with `initial_epoch`,\n",
            " |              `epochs` is to be understood as \"final epoch\".\n",
            " |              The model is not trained for a number of iterations\n",
            " |              given by `epochs`, but merely until the epoch\n",
            " |              of index `epochs` is reached.\n",
            " |          verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training and validation\n",
            " |              (if ).\n",
            " |              See [callbacks](/callbacks).\n",
            " |          validation_split: Float between 0 and 1.\n",
            " |              Fraction of the training data to be used as validation data.\n",
            " |              The model will set apart this fraction of the training data,\n",
            " |              will not train on it, and will evaluate\n",
            " |              the loss and any model metrics\n",
            " |              on this data at the end of each epoch.\n",
            " |              The validation data is selected from the last samples\n",
            " |              in the `x` and `y` data provided, before shuffling.\n",
            " |              This argument is not supported when `x` is a generator or\n",
            " |              `Sequence` instance.\n",
            " |          validation_data: Data on which to evaluate\n",
            " |              the loss and any model metrics at the end of each epoch.\n",
            " |              The model will not be trained on this data.\n",
            " |              `validation_data` will override `validation_split`.\n",
            " |              `validation_data` could be:\n",
            " |                  - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
            " |                  - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
            " |                  - dataset or a dataset iterator\n",
            " |              For the first two cases, `batch_size` must be provided.\n",
            " |              For the last case, `validation_steps` must be provided.\n",
            " |          shuffle: Boolean (whether to shuffle the training data\n",
            " |              before each epoch) or str (for 'batch').\n",
            " |              'batch' is a special option for dealing with the\n",
            " |              limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
            " |              Has no effect when `steps_per_epoch` is not `None`.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers)\n",
            " |              to a weight (float) value, used for weighting the loss function\n",
            " |              (during training only).\n",
            " |              This can be useful to tell the model to\n",
            " |              \"pay more attention\" to samples from\n",
            " |              an under-represented class.\n",
            " |          sample_weight: Optional Numpy array of weights for\n",
            " |              the training samples, used for weighting the loss function\n",
            " |              (during training only). You can either pass a flat (1D)\n",
            " |              Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples),\n",
            " |              or in the case of temporal data,\n",
            " |              you can pass a 2D array with shape\n",
            " |              `(samples, sequence_length)`,\n",
            " |              to apply a different weight to every timestep of every sample.\n",
            " |              In this case you should make sure to specify\n",
            " |              `sample_weight_mode=\"temporal\"` in `compile()`. This argument\n",
            " |              is not supported when `x` generator, or `Sequence` instance,\n",
            " |              instead provide the sample_weights as the third element of `x`.\n",
            " |          initial_epoch: Integer.\n",
            " |              Epoch at which to start training\n",
            " |              (useful for resuming a previous training run).\n",
            " |          steps_per_epoch: Integer or `None`.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              before declaring one epoch finished and starting the\n",
            " |              next epoch. When training with input tensors such as\n",
            " |              TensorFlow data tensors, the default `None` is equal to\n",
            " |              the number of samples in your dataset divided by\n",
            " |              the batch size, or 1 if that cannot be determined.\n",
            " |          validation_steps: Only relevant if `steps_per_epoch`\n",
            " |              is specified. Total number of steps (batches of samples)\n",
            " |              to validate before stopping.\n",
            " |          validation_steps: Only relevant if `validation_data` is provided\n",
            " |              and is a generator. Total number of steps (batches of samples)\n",
            " |              to draw before stopping when performing validation at the end\n",
            " |              of every epoch.\n",
            " |          validation_freq: Only relevant if validation data is provided. Integer\n",
            " |              or list/tuple/set. If an integer, specifies how many training\n",
            " |              epochs to run before a new validation run is performed, e.g.\n",
            " |              `validation_freq=2` runs validation every 2 epochs. If a list,\n",
            " |              tuple, or set, specifies the epochs on which to run validation,\n",
            " |              e.g. `validation_freq=[1, 2, 10]` runs validation at the end\n",
            " |              of the 1st, 2nd, and 10th epochs.\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up\n",
            " |              when using process-based threading. If unspecified, `workers`\n",
            " |              will default to 1. If 0, will execute the generator on the main\n",
            " |              thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |          **kwargs: Used for backwards compatibility.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A `History` object. Its `History.history` attribute is\n",
            " |          a record of training loss values and metrics values\n",
            " |          at successive epochs, as well as validation loss values\n",
            " |          and validation metrics values (if applicable).\n",
            " |      \n",
            " |      # Raises\n",
            " |          RuntimeError: If the model was never compiled.\n",
            " |          ValueError: In case of mismatch between the provided input data\n",
            " |              and what the model expects.\n",
            " |  \n",
            " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
            " |      Trains the model on data generated batch-by-batch by a Python generator\n",
            " |      (or an instance of `Sequence`).\n",
            " |      \n",
            " |      The generator is run in parallel to the model, for efficiency.\n",
            " |      For instance, this allows you to do real-time data augmentation\n",
            " |      on images on CPU in parallel to training your model on GPU.\n",
            " |      \n",
            " |      The use of `keras.utils.Sequence` guarantees the ordering\n",
            " |      and guarantees the single use of every input per epoch when\n",
            " |      using `use_multiprocessing=True`.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          generator: A generator or an instance of `Sequence`\n",
            " |              (`keras.utils.Sequence`) object in order to avoid\n",
            " |              duplicate data when using multiprocessing.\n",
            " |              The output of the generator must be either\n",
            " |              - a tuple `(inputs, targets)`\n",
            " |              - a tuple `(inputs, targets, sample_weights)`.\n",
            " |              This tuple (a single output of the generator) makes a single\n",
            " |              batch. Therefore, all arrays in this tuple must have the same\n",
            " |              length (equal to the size of this batch). Different batches may\n",
            " |              have different sizes. For example, the last batch of the epoch\n",
            " |              is commonly smaller than the others, if the size of the dataset\n",
            " |              is not divisible by the batch size.\n",
            " |              The generator is expected to loop over its data\n",
            " |              indefinitely. An epoch finishes when `steps_per_epoch`\n",
            " |              batches have been seen by the model.\n",
            " |          steps_per_epoch: Integer.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              to yield from `generator` before declaring one epoch\n",
            " |              finished and starting the next epoch. It should typically\n",
            " |              be equal to `ceil(num_samples / batch_size)`\n",
            " |              Optional for `Sequence`: if unspecified, will use\n",
            " |              the `len(generator)` as a number of steps.\n",
            " |          epochs: Integer. Number of epochs to train the model.\n",
            " |              An epoch is an iteration over the entire data provided,\n",
            " |              as defined by `steps_per_epoch`.\n",
            " |              Note that in conjunction with `initial_epoch`,\n",
            " |              `epochs` is to be understood as \"final epoch\".\n",
            " |              The model is not trained for a number of iterations\n",
            " |              given by `epochs`, but merely until the epoch\n",
            " |              of index `epochs` is reached.\n",
            " |          verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training.\n",
            " |              See [callbacks](/callbacks).\n",
            " |          validation_data: This can be either\n",
            " |              - a generator or a `Sequence` object for the validation data\n",
            " |              - tuple `(x_val, y_val)`\n",
            " |              - tuple `(x_val, y_val, val_sample_weights)`\n",
            " |              on which to evaluate\n",
            " |              the loss and any model metrics at the end of each epoch.\n",
            " |              The model will not be trained on this data.\n",
            " |          validation_steps: Only relevant if `validation_data`\n",
            " |              is a generator. Total number of steps (batches of samples)\n",
            " |              to yield from `validation_data` generator before stopping\n",
            " |              at the end of every epoch. It should typically\n",
            " |              be equal to the number of samples of your\n",
            " |              validation dataset divided by the batch size.\n",
            " |              Optional for `Sequence`: if unspecified, will use\n",
            " |              the `len(validation_data)` as a number of steps.\n",
            " |          validation_freq: Only relevant if validation data is provided. Integer\n",
            " |              or `collections.Container` instance (e.g. list, tuple, etc.). If an\n",
            " |              integer, specifies how many training epochs to run before a new\n",
            " |              validation run is performed, e.g. `validation_freq=2` runs\n",
            " |              validation every 2 epochs. If a Container, specifies the epochs on\n",
            " |              which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            " |              validation at the end of the 1st, 2nd, and 10th epochs.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers)\n",
            " |              to a weight (float) value, used for weighting the loss function\n",
            " |              (during training only). This can be useful to tell the model to\n",
            " |              \"pay more attention\" to samples\n",
            " |              from an under-represented class.\n",
            " |          max_queue_size: Integer. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Maximum number of processes to spin up\n",
            " |              when using process-based threading.\n",
            " |              If unspecified, `workers` will default to 1. If 0, will\n",
            " |              execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean.\n",
            " |              If `True`, use process-based threading.\n",
            " |              If unspecified, `use_multiprocessing` will default to `False`.\n",
            " |              Note that because this implementation\n",
            " |              relies on multiprocessing,\n",
            " |              you should not pass non-picklable arguments to the generator\n",
            " |              as they can't be passed easily to children processes.\n",
            " |          shuffle: Boolean. Whether to shuffle the order of the batches at\n",
            " |              the beginning of each epoch. Only used with instances\n",
            " |              of `Sequence` (`keras.utils.Sequence`).\n",
            " |              Has no effect when `steps_per_epoch` is not `None`.\n",
            " |          initial_epoch: Integer.\n",
            " |              Epoch at which to start training\n",
            " |              (useful for resuming a previous training run).\n",
            " |      \n",
            " |      # Returns\n",
            " |          A `History` object. Its `History.history` attribute is\n",
            " |          a record of training loss values and metrics values\n",
            " |          at successive epochs, as well as validation loss values\n",
            " |          and validation metrics values (if applicable).\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case the generator yields data in an invalid format.\n",
            " |      \n",
            " |      # Example\n",
            " |      \n",
            " |      ```python\n",
            " |      def generate_arrays_from_file(path):\n",
            " |          while True:\n",
            " |              with open(path) as f:\n",
            " |                  for line in f:\n",
            " |                      # create numpy arrays of input data\n",
            " |                      # and labels, from each line in the file\n",
            " |                      x1, x2, y = process_line(line)\n",
            " |                      yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
            " |      \n",
            " |      model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
            " |                          steps_per_epoch=10000, epochs=10)\n",
            " |      ```\n",
            " |  \n",
            " |  predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Generates output predictions for the input samples.\n",
            " |      \n",
            " |      Computation is done in batches.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: Input data. It could be:\n",
            " |              - A Numpy array (or array-like), or a list of arrays\n",
            " |                (in case the model has multiple inputs).\n",
            " |              - A dict mapping input names to the corresponding\n",
            " |                array/tensors, if the model has named inputs.\n",
            " |              - A generator or `keras.utils.Sequence` returning\n",
            " |                `(inputs, targets)` or `(inputs, targets, sample weights)`.\n",
            " |              - None (default) if feeding from framework-native\n",
            " |                tensors (e.g. TensorFlow data tensors).\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per gradient update.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` is your data is in the\n",
            " |              form of symbolic tensors, generators, or\n",
            " |              `keras.utils.Sequence` instances (since they generate batches).\n",
            " |          verbose: Verbosity mode, 0 or 1.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              before declaring the prediction round finished.\n",
            " |              Ignored with the default value of `None`.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during prediction.\n",
            " |              See [callbacks](/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up when using\n",
            " |              process-based threading. If unspecified, `workers` will default\n",
            " |              to 1. If 0, will execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case of mismatch between the provided\n",
            " |              input data and the model's expectations,\n",
            " |              or in case a stateful model receives a number of samples\n",
            " |              that is not a multiple of the batch size.\n",
            " |  \n",
            " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Generates predictions for the input samples from a data generator.\n",
            " |      \n",
            " |      The generator should return the same kind of data as accepted by\n",
            " |      `predict_on_batch`.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          generator: Generator yielding batches of input samples\n",
            " |              or an instance of Sequence (keras.utils.Sequence)\n",
            " |              object in order to avoid duplicate data\n",
            " |              when using multiprocessing.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              to yield from `generator` before stopping.\n",
            " |              Optional for `Sequence`: if unspecified, will use\n",
            " |              the `len(generator)` as a number of steps.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training.\n",
            " |              See [callbacks](/callbacks).\n",
            " |          max_queue_size: Maximum size for the generator queue.\n",
            " |          workers: Integer. Maximum number of processes to spin up\n",
            " |              when using process based threading.\n",
            " |              If unspecified, `workers` will default to 1. If 0, will\n",
            " |              execute the generator on the main thread.\n",
            " |          use_multiprocessing: If `True`, use process based threading.\n",
            " |              Note that because\n",
            " |              this implementation relies on multiprocessing,\n",
            " |              you should not pass\n",
            " |              non picklable arguments to the generator\n",
            " |              as they can't be passed\n",
            " |              easily to children processes.\n",
            " |          verbose: verbosity mode, 0 or 1.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case the generator yields\n",
            " |              data in an invalid format.\n",
            " |  \n",
            " |  predict_on_batch(self, x)\n",
            " |      Returns predictions for a single batch of samples.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: Input samples, as a Numpy array.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Numpy array(s) of predictions.\n",
            " |  \n",
            " |  test_on_batch(self, x, y, sample_weight=None)\n",
            " |      Test the model on a single batch of samples.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: Numpy array of test data,\n",
            " |              or list of Numpy arrays if the model has multiple inputs.\n",
            " |              If all inputs in the model are named,\n",
            " |              you can also pass a dictionary\n",
            " |              mapping input names to Numpy arrays.\n",
            " |          y: Numpy array of target data,\n",
            " |              or list of Numpy arrays if the model has multiple outputs.\n",
            " |              If all outputs in the model are named,\n",
            " |              you can also pass a dictionary\n",
            " |              mapping output names to Numpy arrays.\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |              weights to apply to the model's loss for each sample.\n",
            " |              In the case of temporal data, you can pass a 2D array\n",
            " |              with shape (samples, sequence_length),\n",
            " |              to apply a different weight to every timestep of every sample.\n",
            " |              In this case you should make sure to specify\n",
            " |              sample_weight_mode=\"temporal\" in compile().\n",
            " |      \n",
            " |      # Returns\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |  \n",
            " |  train_on_batch(self, x, y, sample_weight=None, class_weight=None)\n",
            " |      Runs a single gradient update on a single batch of data.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          x: Numpy array of training data,\n",
            " |              or list of Numpy arrays if the model has multiple inputs.\n",
            " |              If all inputs in the model are named,\n",
            " |              you can also pass a dictionary\n",
            " |              mapping input names to Numpy arrays.\n",
            " |          y: Numpy array of target data,\n",
            " |              or list of Numpy arrays if the model has multiple outputs.\n",
            " |              If all outputs in the model are named,\n",
            " |              you can also pass a dictionary\n",
            " |              mapping output names to Numpy arrays.\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |              weights to apply to the model's loss for each sample.\n",
            " |              In the case of temporal data, you can pass a 2D array\n",
            " |              with shape (samples, sequence_length),\n",
            " |              to apply a different weight to every timestep of every sample.\n",
            " |              In this case you should make sure to specify\n",
            " |              sample_weight_mode=\"temporal\" in compile().\n",
            " |          class_weight: Optional dictionary mapping\n",
            " |              class indices (integers) to\n",
            " |              a weight (float) to apply to the model's loss for the samples\n",
            " |              from this class during training.\n",
            " |              This can be useful to tell the model to \"pay more attention\" to\n",
            " |              samples from an under-represented class.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Scalar training loss\n",
            " |          (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.engine.network.Network:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  call(self, inputs, mask=None)\n",
            " |      Calls the model on new inputs.\n",
            " |      \n",
            " |      In this case `call` just reapplies\n",
            " |      all ops in the graph to the new inputs\n",
            " |      (e.g. build a new computational graph from the provided inputs).\n",
            " |      \n",
            " |      A model is callable on non-Keras tensors.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: A tensor or list of tensors.\n",
            " |          mask: A mask or list of masks. A mask can be\n",
            " |              either a tensor or None (no mask).\n",
            " |      \n",
            " |      # Returns\n",
            " |          A tensor if there is a single output, or\n",
            " |          a list of tensors if there are more than one outputs.\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      # Returns\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      Assumes that the layer will be built\n",
            " |      to match that input shape provided.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  get_layer(self, name=None, index=None)\n",
            " |      Retrieves a layer based on either its name (unique) or index.\n",
            " |      \n",
            " |      If `name` and `index` are both provided, `index` will take precedence.\n",
            " |      \n",
            " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
            " |      \n",
            " |      # Arguments\n",
            " |          name: String, name of layer.\n",
            " |          index: Integer, index of layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A layer instance.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case of invalid layer name or index.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Retrieves the weights of the model.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A flat list of Numpy arrays.\n",
            " |  \n",
            " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, reshape=False)\n",
            " |      Loads all layer weights from a HDF5 save file.\n",
            " |      \n",
            " |      If `by_name` is False (default) weights are loaded\n",
            " |      based on the network's topology, meaning the architecture\n",
            " |      should be the same as when the weights were saved.\n",
            " |      Note that layers that don't have weights are not taken\n",
            " |      into account in the topological ordering, so adding or\n",
            " |      removing layers is fine as long as they don't have weights.\n",
            " |      \n",
            " |      If `by_name` is True, weights are loaded into layers\n",
            " |      only if they share the same name. This is useful\n",
            " |      for fine-tuning or transfer-learning models where\n",
            " |      some of the layers have changed.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          filepath: String, path to the weights file to load.\n",
            " |          by_name: Boolean, whether to load weights by name\n",
            " |              or by topological order.\n",
            " |          skip_mismatch: Boolean, whether to skip loading of layers\n",
            " |              where there is a mismatch in the number of weights,\n",
            " |              or a mismatch in the shape of the weight\n",
            " |              (only valid when `by_name`=True).\n",
            " |          reshape: Reshape weights to fit the layer when the correct number\n",
            " |              of weight arrays is present but their shape does not match.\n",
            " |      \n",
            " |      \n",
            " |      # Raises\n",
            " |          ImportError: If h5py is not available.\n",
            " |  \n",
            " |  reset_states(self)\n",
            " |  \n",
            " |  run_internal_graph(self, inputs, masks=None)\n",
            " |      Computes output tensors for new inputs.\n",
            " |      \n",
            " |      # Note:\n",
            " |          - Expects `inputs` to be a list (potentially with 1 element).\n",
            " |          - Can be run on non-Keras tensors.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: List of tensors\n",
            " |          masks: List of masks (tensors or None).\n",
            " |      \n",
            " |      # Returns\n",
            " |          Three lists: output_tensors, output_masks, output_shapes\n",
            " |  \n",
            " |  save(self, filepath, overwrite=True, include_optimizer=True)\n",
            " |      Saves the model to a single HDF5 file.\n",
            " |      \n",
            " |      The savefile includes:\n",
            " |          - The model architecture, allowing to re-instantiate the model.\n",
            " |          - The model weights.\n",
            " |          - The state of the optimizer, allowing to resume training\n",
            " |              exactly where you left off.\n",
            " |      \n",
            " |      This allows you to save the entirety of the state of a model\n",
            " |      in a single file.\n",
            " |      \n",
            " |      Saved models can be reinstantiated via `keras.models.load_model`.\n",
            " |      The model returned by `load_model`\n",
            " |      is a compiled model ready to be used (unless the saved model\n",
            " |      was never compiled in the first place).\n",
            " |      \n",
            " |      # Arguments\n",
            " |          filepath: one of the following:\n",
            " |              - string, path to the file to save the model to\n",
            " |              - h5py.File or h5py.Group object where to save the model\n",
            " |              - any file-like object implementing the method `write` that accepts\n",
            " |                  `bytes` data (e.g. `io.BytesIO`).\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          include_optimizer: If True, save optimizer's state together.\n",
            " |      \n",
            " |      # Example\n",
            " |      \n",
            " |      ```python\n",
            " |      from keras.models import load_model\n",
            " |      \n",
            " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
            " |      del model  # deletes the existing model\n",
            " |      \n",
            " |      # returns a compiled model\n",
            " |      # identical to the previous one\n",
            " |      model = load_model('my_model.h5')\n",
            " |      ```\n",
            " |  \n",
            " |  save_weights(self, filepath, overwrite=True)\n",
            " |      Dumps all layer weights to a HDF5 file.\n",
            " |      \n",
            " |      The weight file has:\n",
            " |          - `layer_names` (attribute), a list of strings\n",
            " |              (ordered names of model layers).\n",
            " |          - For every layer, a `group` named `layer.name`\n",
            " |              - For every such layer group, a group attribute `weight_names`,\n",
            " |                  a list of strings\n",
            " |                  (ordered names of weights tensor of the layer).\n",
            " |              - For every weight in the layer, a dataset\n",
            " |                  storing the weight value, named after the weight tensor.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          filepath: String, path to the file to save the weights to.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ImportError: If h5py is not available.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the model.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          weights: A list of Numpy arrays with shapes and types matching\n",
            " |              the output of `model.get_weights()`.\n",
            " |  \n",
            " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
            " |      Prints a string summary of the network.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          line_length: Total length of printed lines\n",
            " |              (e.g. set this to adapt the display to different\n",
            " |              terminal window sizes).\n",
            " |          positions: Relative or absolute positions of log elements\n",
            " |              in each line. If not provided,\n",
            " |              defaults to `[.33, .55, .67, 1.]`.\n",
            " |          print_fn: Print function to use.\n",
            " |              It will be called on each line of the summary.\n",
            " |              You can set it to a custom function\n",
            " |              in order to capture the string summary.\n",
            " |              It defaults to `print` (prints to stdout).\n",
            " |  \n",
            " |  to_json(self, **kwargs)\n",
            " |      Returns a JSON string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a JSON save file, use\n",
            " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `json.dumps()`.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A JSON string.\n",
            " |  \n",
            " |  to_yaml(self, **kwargs)\n",
            " |      Returns a yaml string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a yaml save file, use\n",
            " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
            " |      \n",
            " |      `custom_objects` should be a dictionary mapping\n",
            " |      the names of custom losses / layers / etc to the corresponding\n",
            " |      functions / classes.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `yaml.dump()`.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A YAML string.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.engine.network.Network:\n",
            " |  \n",
            " |  input_spec\n",
            " |      Gets the model's input specs.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A list of `InputSpec` instances (one per input to the model)\n",
            " |              or a single instance if the model has only one input.\n",
            " |  \n",
            " |  losses\n",
            " |      Retrieves the model's losses.\n",
            " |      \n",
            " |      Will only include losses that are either\n",
            " |      unconditional, or conditional on inputs to this model\n",
            " |      (e.g. will not include losses that depend on tensors\n",
            " |      that aren't inputs to this model).\n",
            " |      \n",
            " |      # Returns\n",
            " |          A list of loss tensors.\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |  \n",
            " |  state_updates\n",
            " |      Returns the `updates` from all layers that are stateful.\n",
            " |      \n",
            " |      This is useful for separating training updates and\n",
            " |      state updates, e.g. when we need to update a layer's internal state\n",
            " |      during prediction.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A list of update ops.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  trainable_weights\n",
            " |  \n",
            " |  updates\n",
            " |      Retrieves the model's updates.\n",
            " |      \n",
            " |      Will only include updates that are either\n",
            " |      unconditional, or conditional on inputs to this model\n",
            " |      (e.g. will not include updates that depend on tensors\n",
            " |      that aren't inputs to this model).\n",
            " |      \n",
            " |      # Returns\n",
            " |          A list of update ops.\n",
            " |  \n",
            " |  uses_learning_phase\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, inputs, **kwargs)\n",
            " |      Wrapper around self.call(), for handling internal references.\n",
            " |      \n",
            " |      If a Keras tensor is passed:\n",
            " |          - We call self._add_inbound_node().\n",
            " |          - If necessary, we `build` the layer to match\n",
            " |              the _keras_shape of the input(s).\n",
            " |          - We update the _keras_shape of every input tensor with\n",
            " |              its new shape (obtained via self.compute_output_shape).\n",
            " |              This is done as part of _add_inbound_node().\n",
            " |          - We update the _keras_history of the output tensor(s)\n",
            " |              with the current layer.\n",
            " |              This is done as part of _add_inbound_node().\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: Can be a tensor or list/tuple of tensors.\n",
            " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output of the layer's `call` method.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: in case the layer is missing shape information\n",
            " |              for its `build` call.\n",
            " |  \n",
            " |  add_loss(self, losses, inputs=None)\n",
            " |      Adds losses to the layer.\n",
            " |      \n",
            " |      The loss may potentially be conditional on some inputs tensors,\n",
            " |      for instance activity losses are conditional on the layer's inputs.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          losses: loss tensor or list of loss tensors\n",
            " |              to add to the layer.\n",
            " |          inputs: input tensor or list of inputs tensors to mark\n",
            " |              the losses as conditional on these inputs.\n",
            " |              If None is passed, the loss is assumed unconditional\n",
            " |              (e.g. L2 weight regularization, which only depends\n",
            " |              on the layer's weights variables, not on any inputs tensors).\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Adds updates to the layer.\n",
            " |      \n",
            " |      The updates may potentially be conditional on some inputs tensors,\n",
            " |      for instance batch norm updates are conditional on the layer's inputs.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          updates: update op or list of update ops\n",
            " |              to add to the layer.\n",
            " |          inputs: input tensor or list of inputs tensors to mark\n",
            " |              the updates as conditional on these inputs.\n",
            " |              If None is passed, the updates are assumed unconditional.\n",
            " |  \n",
            " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
            " |      Adds a weight variable to the layer.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          name: String, the name for the weight variable.\n",
            " |          shape: The shape tuple of the weight.\n",
            " |          dtype: The dtype of the weight.\n",
            " |          initializer: An Initializer instance (callable).\n",
            " |          regularizer: An optional Regularizer instance.\n",
            " |          trainable: A boolean, whether the weight should\n",
            " |              be trained via backprop or not (assuming\n",
            " |              that the layer itself is also trainable).\n",
            " |          constraint: An optional Constraint instance.\n",
            " |      \n",
            " |      # Returns\n",
            " |          The created weight variable.\n",
            " |  \n",
            " |  assert_input_compatibility(self, inputs)\n",
            " |      Checks compatibility between the layer and provided inputs.\n",
            " |      \n",
            " |      This checks that the tensor(s) `input`\n",
            " |      verify the input assumptions of the layer\n",
            " |      (if any). If not, exceptions are raised.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: input tensor or list of input tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: in case of mismatch between\n",
            " |              the provided inputs and the expectations of the layer.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Counts the total number of scalars composing the weights.\n",
            " |      \n",
            " |      # Returns\n",
            " |          An integer count.\n",
            " |      \n",
            " |      # Raises\n",
            " |          RuntimeError: if the layer isn't yet built\n",
            " |              (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  built\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape tuple(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Input shape tuple\n",
            " |          (or list of input shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output tensor or list of output tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape tuple(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one inbound node,\n",
            " |      or if all inbound nodes have the same output shape.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output shape tuple\n",
            " |          (or list of input shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  weights\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd_wsllt_HyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION TO CREATE 1D DATA INTO TIME SERIES DATASET\n",
        "\n",
        "def new_dataset(dataset, step_size):\n",
        "  data_X, data_Y = [], []\n",
        "  for i in range(len(dataset)-step_size-1):\n",
        "    a = dataset[i:(i+step_size), 0]\n",
        "    data_X.append(a)\n",
        "    data_Y.append(dataset[i + step_size, 0])\n",
        "  return np.array(data_X), np.array(data_Y)\n",
        "\n",
        "# THIS FUNCTION CAN BE USED TO CREATE A TIME SERIES DATASET FROM ANY 1D ARRAY\t\n",
        "\n",
        "\n",
        "# TIME-SERIES DATASET (FOR TIME T, VALUES FOR TIME T+1)\n",
        "trainX, trainY = new_dataset(train_OHLC, 1)\n",
        "testX, testY = new_dataset(test_OHLC, 1)\n",
        "\n",
        "# RESHAPING TRAIN AND TEST DATA\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "step_size = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8teQB-MPCBbo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "60689fa5-1da7-4511-a0eb-2fa35b81e08d"
      },
      "source": [
        "# LSTM MODEL Orignal\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(1, step_size), return_sequences = True))\n",
        "model.add(LSTM(16))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "# # LSTM MODEL\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(64, return_sequences = True))\n",
        "# model.add(LSTM(32))\n",
        "# model.add(LSTM(32))\n",
        "# model.add(LSTM(16))\n",
        "# model.add(Dense(1))\n",
        "# model.add(Activation('linear'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_20 (LSTM)               (None, 1, 32)             4352      \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, 16)                3136      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 17        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 7,505\n",
            "Trainable params: 7,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7OKcEvaCH-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # MODEL COMPILING AND TRAINING\n",
        "# model.compile(loss='mean_squared_error', optimizer='adagrad') # Try SGD, adam, adagrad and compare!!!\n",
        "# model.fit(trainX, trainY, epochs=5, batch_size=1, verbose=2)\n",
        "\n",
        "# MODEL COMPILING AND TRAINING\n",
        "model.compile(loss='mean_squared_error', optimizer='adagrad') # Try SGD, adam, adagrad and compare!!!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCzqGuBaDBwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "e0f3f85c-991f-4ed6-8ac1-ddefb91cc723"
      },
      "source": [
        "model.fit(trainX, trainY, epochs=5, batch_size=1, verbose=2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 5s - loss: 0.0062\n",
            "Epoch 2/5\n",
            " - 4s - loss: 3.4269e-04\n",
            "Epoch 3/5\n",
            " - 4s - loss: 3.0338e-04\n",
            "Epoch 4/5\n",
            " - 4s - loss: 2.7157e-04\n",
            "Epoch 5/5\n",
            " - 4s - loss: 2.4636e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa994183630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkLydnSCC51E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PREDICTION\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAy6uNGcKW7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DE-NORMALIZING FOR PLOTTING\n",
        "trainPredict = scaler.inverse_transform(trainPredict)\n",
        "trainY = scaler.inverse_transform([trainY])\n",
        "testPredict = scaler.inverse_transform(testPredict)\n",
        "testY = scaler.inverse_transform([testY])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10al4yysAiHN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe9a9160-6e20-4ad0-9494-b1879ae5c139"
      },
      "source": [
        "# TRAINING RMSE\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "print('Train RMSE: %.2f' % (trainScore))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train RMSE: 1.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K5Ks0OBKeTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d36abe6a-4d3e-4395-e6b9-e0045aacef7e"
      },
      "source": [
        "# TEST RMSE\n",
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Test RMSE: %.2f' % (testScore))\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 2.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-o22kmyKegd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "0370271d-88dc-4cfd-f2ad-95ce6cef7de7"
      },
      "source": [
        "# CREATING SIMILAR DATASET TO PLOT TRAINING PREDICTIONS\n",
        "trainPredictPlot = np.empty_like(OHLC_avg)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[step_size:len(trainPredict)+step_size, :] = trainPredict\n",
        "\n",
        "# CREATING SIMILAR DATASSET TO PLOT TEST PREDICTIONS\n",
        "testPredictPlot = np.empty_like(OHLC_avg)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(trainPredict)+(step_size*2)+1:len(OHLC_avg)-1, :] = testPredict\n",
        "\n",
        "# DE-NORMALIZING MAIN DATASET \n",
        "OHLC_avg = scaler.inverse_transform(OHLC_avg)\n",
        "\n",
        "# PLOT OF MAIN OHLC VALUES, TRAIN PREDICTIONS AND TEST PREDICTIONS\n",
        "plt.plot(OHLC_avg, 'g', label = 'original dataset')\n",
        "plt.plot(trainPredictPlot, 'r', label = 'training set')\n",
        "plt.plot(testPredictPlot, 'b', label = 'predicted stock price/test set')\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.xlabel('Time in Days')\n",
        "plt.ylabel('OHLC Value of Apple Stocks')\n",
        "plt.show()\n",
        "\n",
        "# PREDICT FUTURE VALUES\n",
        "last_val = testPredict[-1]\n",
        "last_val_scaled = last_val/last_val\n",
        "next_val = model.predict(np.reshape(last_val_scaled, (1,1,1)))\n",
        "print (\"Last Day Value:\", np.asscalar(last_val))\n",
        "print (\"Next Day Value:\", np.asscalar(last_val*next_val))\n",
        "# print np.append(last_val, next_val)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxuSxRsEYY0ye/wdKrkvHFiqMdQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Last Day Value: 166.97572326660156\n",
            "Next Day Value: 176.8380889892578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}